{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这是我的第一个练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 假定变量存在固定的概率分布，并且预测该分部"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 对话机器人，天气预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 确定概率分布？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:计算一句语言序列的概率的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 文本生成相关的，比如机器翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 每个词的出现都是独立的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 好：约束力小，词更容易出现，统计结果更可靠 坏处：辨别力更弱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 基于马尔科夫假设，词语的出现与前1个词有关的语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设计句子生成器\n",
    "\n",
    "### 首先我们定义语法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "daughter = '''\n",
    "daughter = 女儿说: 爷爷 , noun_phase end punc daughter*\n",
    "daughter* = 爷爷 , noun_phase end punc daughter* | none\n",
    "noun_phase = 奶油面包真的好好吃\n",
    "end = 呀 | 啊\n",
    "punc = ! | !!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = '''\n",
    "parents = parents* time activity time activity time activity end\n",
    "parents* = 父亲说: 我们 | 母亲说: 我们\n",
    "time = 二号 | 晚上 | 三号\n",
    "activity = 去听经, | 住旅店, | 去餐厅,\n",
    "end = 然后看电影。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三个语法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''\n",
    "question = 警官说: 你们 case pastday 干什么去了?\n",
    "case = 案件前 | 案件发生前\n",
    "pastday = 几天 | 这几天 | 那几天\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammer(grammer_str, split='=', line_split='\\n'):\n",
    "    grammer = {}\n",
    "    for line in grammer_str.split(line_split):\n",
    "        if line.strip():\n",
    "            title, words = line.split(split)\n",
    "            grammer[title.strip()] = [word.split() for word in words.split(\"|\")]\n",
    "    return grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daughter', 'daughter*', 'noun_phase', 'end', 'punc']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parents_grammer = create_grammer(parents)      \n",
    "question_grammer = create_grammer(question)\n",
    "daughter_grammer = create_grammer(daughter)\n",
    "list(daughter_grammer.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'警官说:你们案件前这几天干什么去了?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def generate(grammer, target):\n",
    "    if target not in grammer:\n",
    "        return target\n",
    "#     for next_t in random.choice(grammer[target]):\n",
    "#         print(next_t)\n",
    "#     print(grammer[target])\n",
    "    words_list = [generate(grammer,next_t) for next_t in random.choice(grammer[target])]\n",
    "#     return words_list\n",
    "    sentence = \"\".join([word for word in words_list if word != 'none'])\n",
    "    return sentence\n",
    "generate(daughter_grammer,\"daughter\")\n",
    "generate(parents_grammer,\"parents\")\n",
    "generate(question_grammer,\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['警官说:你们案件前这几天干什么去了?女儿说:爷爷,奶油面包真的好好吃呀!爷爷,奶油面包真的好好吃呀!母亲说:我们二号住旅店,三号住旅店,三号去餐厅,然后看电影。',\n",
       " '警官说:你们案件发生前前几天干什么去了?女儿说:爷爷,奶油面包真的好好吃啊!!爷爷,奶油面包真的好好吃呀!爷爷,奶油面包真的好好吃啊!!爷爷,奶油面包真的好好吃啊!母亲说:我们晚上去餐厅,晚上去餐厅,三号住旅店,然后看电影。',\n",
       " '警官说:你们案件前前几天干什么去了?女儿说:爷爷,奶油面包真的好好吃呀!父亲说:我们晚上去听经,三号住旅店,三号去餐厅,然后看电影。']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_n(*args, n):\n",
    "    inters = []\n",
    "    for _ in range(n):\n",
    "        interrogation = \"\"\n",
    "        for grammer in args:\n",
    "            interrogation += generate(grammer, list(grammer.keys())[0])\n",
    "        inters.append(interrogation)\n",
    "    return inters\n",
    "generate_n(question_grammer, daughter_grammer, parents_grammer, n = 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duxy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "['吴京', '意淫', '到', '了', '脑残']\n",
      "['吴京意淫', '意淫到', '到了', '了脑残', '脑残的']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "def token(string):\n",
    "    # we will learn the regular expression next course.\n",
    "    return re.findall('\\w+', string)\n",
    "\n",
    "def cut(string): \n",
    "    return jieba.lcut(string)\n",
    "\n",
    "# 清洗评论数据\n",
    "all_comment = data['comment'].tolist()\n",
    "clean_comment = [''.join(token(str(comment))) for comment in all_comment]\n",
    "with open('douban_comments.txt', 'w') as f:\n",
    "    for a in clean_comment:\n",
    "        f.write(a + '\\n')\n",
    "\n",
    "# 统计词频\n",
    "TOKEN=[]\n",
    "TOKEN_2_GRAM  = []\n",
    "for i, line in enumerate((open('douban_comments.txt'))):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    # replace 10000 with a big number when you do your homework. \n",
    "\n",
    "    if i > 100000: \n",
    "        break \n",
    "    cut_line = cut(line)\n",
    "    [1,2,3,4]\n",
    "    TOKEN += cut_line\n",
    "    TOKEN_2_GRAM += [cut_line[i]+cut_line[i+1] for i in range(len(cut_line)-1)]\n",
    "\n",
    "print(TOKEN[:5])\n",
    "print(TOKEN_2_GRAM[:5])\n",
    "\n",
    "\n",
    "word_freq = Counter(TOKEN)\n",
    "freq_2_word = Counter(TOKEN_2_GRAM)\n",
    "\n",
    "def prob_1(word): return word_freq[word] / len(TOKEN)\n",
    "\n",
    "def prob_2(word1, word2):  # p(w1,w2) = count(w1,2)/count(w1)\n",
    "    if word1 + word2 in TOKEN_2_GRAM: return freq_2_word[word1+word2] / word_freq[word1]\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00014239954020884923\n",
      "0.02702702702702703\n",
      "5.419378614048113e-07\n"
     ]
    }
   ],
   "source": [
    "print(prob_1(\"吴京\")) \n",
    "print(prob_2(\"炒作\",\"水平\"))\n",
    "print(prob_2(\"炒菜\",\"水平\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('警官说:你们案件发生前那几天干什么去了?', 5.337733358360071e-60), ('警官说:你们案件发生前几天干什么去了?', 1.560219460648649e-55), ('警官说:你们案件发生前几天干什么去了?', 1.560219460648649e-55), ('警官说:你们案件前几天干什么去了?', 2.878963755372707e-49), ('警官说:你们案件前几天干什么去了?', 2.878963755372707e-49)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('警官说:你们案件前几天干什么去了?', 2.878963755372707e-49)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prob(sentence,GRAM):\n",
    "    if GRAM == 2:\n",
    "        words = cut(sentence)\n",
    "        prob = 1\n",
    "        for i in range(len(words)-1):\n",
    "            prob *= prob_2(words[i],words[i+1]) \n",
    "    return prob\n",
    "\n",
    "def generate_best(grammer): # you code here\n",
    "    candidate_sentences = generate_n(grammer, n=5)\n",
    "    sentence_prob = sorted([(sentence, get_prob(sentence,2)) for sentence in candidate_sentences], key=lambda x:x[1])\n",
    "    print(sentence_prob)\n",
    "    return sentence_prob[-1]\n",
    "\n",
    "generate_best(question_grammer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 1.未登陆词的问题未解决 2. 生成语言模式固定，只依赖于预先设定的语法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
