{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这是我的第三个练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动态规划(DP)可以处理的问题有什么特点 ？    \n",
    "**最优解可以由该问题的子问题最优解求得**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单描述动态规划的几个步骤？    \n",
    "**确认子问题，初始化子问题，确定迭代公式，得到最优解**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完成编辑距离解码函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=2**10)\n",
    "def edit_distance(string1, string2):\n",
    "    '''找出string1与string2的最小编辑距离'''\n",
    "    \n",
    "    # 如果两个词有一个为空，则编辑距离就是另外一个词的长度\n",
    "    if len(string1) == 0: return len(string2)\n",
    "    if len(string2) == 0: return len(string1)\n",
    "    \n",
    "    tail_s1 = string1[-1]\n",
    "    tail_s2 = string2[-1]\n",
    "    \n",
    "    # 求删除的string1的尾部与string2的编辑距离\n",
    "    # 求添加string2的尾部至string1的尾部与string2的编辑距离\n",
    "    candidates = [\n",
    "        (edit_distance(string1[:-1], string2) + 1, 'DEL {}'.format(tail_s1)),  \n",
    "        # string 1 delete tail\n",
    "        (edit_distance(string1, string2[:-1]) + 1, 'ADD {}'.format(tail_s2)),  \n",
    "        # string 1 add tail of string2\n",
    "    ]\n",
    "    \n",
    "    # 如果两个尾部一样，说明无需替换尾部，直接计算string1和string2都减去尾部的编辑距离即可\n",
    "    if tail_s1 == tail_s2:\n",
    "        both_forward = (edit_distance(string1[:-1], string2[:-1]) + 0, '')\n",
    "    # 如果两个尾部不一样，说明需要替换string1的尾部成string2的尾部，编辑距离加1\n",
    "    else:\n",
    "        both_forward = (edit_distance(string1[:-1], string2[:-1]) + 1, 'SUB {} => {}'.format(tail_s1, tail_s2))\n",
    "\n",
    "    candidates.append(both_forward)\n",
    "    \n",
    "    # 比较三种可能的操作下，最小的编辑距离和操作\n",
    "    min_distance, operation = min(candidates, key=lambda x: x[0])\n",
    "    \n",
    "    solution[(string1, string2)] = operation \n",
    "    \n",
    "    return min_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拼音自动纠错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def token(string):\n",
    "    '''转换成拼音'''\n",
    "    return pinyin.get(string,format='strip',delimiter=' ')\n",
    "\n",
    "def tokens(text):\n",
    "    \"List all the pinyin characters\"\n",
    "    return re.findall('[a-z]+',text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33425826"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_dataset = \"/Users/duxy/Downloads/Study/NLP/NLP技能班/dateset/article_9k.txt\"\n",
    "all_string = open(chinese_dataset).read()\n",
    "len(all_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129433034"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pinyin = token(all_string)\n",
    "len(all_pinyin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINYIN_COUNT = Counter(tokens(all_pinyin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "1-gram 33336215\n",
      "2-gram 33246606\n"
     ]
    }
   ],
   "source": [
    "gram_1 = []\n",
    "gram_2 = []\n",
    "for i, line in enumerate(open(chinese_dataset)):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    line_pinyin = token(line)\n",
    "    seg_pinnyin = line_pinyin.split()\n",
    "    gram_1 += seg_pinnyin\n",
    "    gram_2 += [seg_pinnyin[index]+\" \"+seg_pinnyin[index+1] for index in range(len(seg_pinnyin)-1)]\n",
    "print(f\"1-gram {len(gram_1)}\")\n",
    "print(f\"2-gram {len(gram_2)}\")\n",
    "    \n",
    "gram1_freq = Counter(gram_1)\n",
    "gram2_freq = Counter(gram_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_1(freq, pinyin):\n",
    "    return freq[pinyin]/len(gram_1) if pinyin in freq else 1/len(gram_1)\n",
    "\n",
    "def pro_2(freq, py1, py2):\n",
    "    if py2 == \"\":\n",
    "        return pro_1(gram1_freq, py1)\n",
    "    elif py1 == \"\":\n",
    "        return pro_1(gram1_freq, py2)\n",
    "    return freq[py1+\" \"+py2]/len(gram_2) if py1+\" \"+py2 in gram2_freq else 1/len(gram_2)\n",
    "\n",
    "def get_sentence_prob(sentence_py):\n",
    "    sentence_py = sentence_py.split()\n",
    "#     print(sentence_py)\n",
    "    prob = 1\n",
    "    for i in range(len(sentence_py)-1):\n",
    "        prob *= pro_2(gram2_freq, sentence_py[i], sentence_py[i+1])\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wi: 2.9997406724188696e-08\n",
      "wo: 0.001939662316192765\n",
      "xiao mi: 1.1249268571955886e-05\n",
      "xi aomi: 3.007825821378579e-08\n",
      "xiao mi shou ji: 4.0288402672042684e-15\n",
      "xia mi shou ji: 2.585352043125734e-16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"wi: {pro_1(gram1_freq, 'wi')}\")\n",
    "print(f\"wo: {pro_1(gram1_freq, 'wo')}\")\n",
    "print(f\"xiao mi: {pro_2(gram2_freq, 'xiao', 'mi')}\")\n",
    "print(f\"xi aomi: {pro_2(gram2_freq, 'xi', 'aomi')}\")\n",
    "print(f\"xiao mi shou ji: {get_sentence_prob('xiao mi shou ji')}\")\n",
    "print(f\"xia mi shou ji: {get_sentence_prob('xia mi shou ji')}\")\n",
    "get_sentence_prob(\"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=2**10)\n",
    "def split_pinyin(string):\n",
    "    if len(string) == 1:\n",
    "        return string, pro_1(gram1_freq,string)\n",
    "    if len(string)<=5:\n",
    "        result = sorted([(string[:i],string[i:],pro_2(gram2_freq, string[:i], string[i:])) for i in range(len(string)-1)])\n",
    "        best_result = result[0]\n",
    "        if best_result[1]==string:\n",
    "            return best_result[1], best_result[2]\n",
    "        string1, prob1 = split_pinyin(best_result[0])\n",
    "        string2, prob2 = split_pinyin(best_result[1])\n",
    "        \n",
    "        return string1 + \" \" + string2, prob1*prob2\n",
    "    else:\n",
    "        candidate = []\n",
    "        for i in range(1,5):\n",
    "            string1, prob1 = split_pinyin(string[:len(string)-5+i])\n",
    "            string2, prob2 = split_pinyin(string[len(string)-5+i:])\n",
    "            candidate.append((string1+\" \"+string2, prob1*prob2))\n",
    "        candidate = sorted(candidate, key=lambda x:x[1])\n",
    "        best_candidate = candidate[-1]\n",
    "\n",
    "        return best_candidate[0], best_candidate[1]\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m0.001s\n",
      "user\t0m0.000s\n",
      "sys\t0m0.000s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('xiao mi shou ji', 4.751797169058313e-10)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!time\n",
    "split_pinyin(\"xiaomishouji\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('zhe shi yi tiao shen qi de tian lu', 6.430052806729468e-21)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_pinyin(\"zheshiyitiaoshenqidetianlu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hua wei niu bi', 1.428563549285516e-10)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_pinyin(\"huaweiniubi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def splits(word):\n",
    "    'Return a list of all possible (first, rest) pairs that comprise pinyin.'\n",
    "    return [(word[:i], word[i:])\n",
    "           for i in range(len(word)+1)]\n",
    "\n",
    "def edits1(word):\n",
    "    'Return all strings that are one edit away from this pinyin.'\n",
    "    pairs = splits(word)   \n",
    "    deletes = [a+b[1:] for (a,b) in pairs if b] # input pinyin -删除操作 去掉b[0]\n",
    "    replaces = [a+c+b[1:] for (a,b) in pairs for c in alphabet if b] #替换  b[0]替换为c\n",
    "    inserts = [a+c+b for (a,b) in pairs for c in alphabet] #插入操作 在a和b之间插入字母\n",
    "    return set(deletes + replaces + inserts)\n",
    "\n",
    "def known(words):\n",
    "    'Return the pinyin in our data'\n",
    "    return {w for w in words if w in PINYIN_COUNT}\n",
    "\n",
    "def edits0(word):\n",
    "    'Return all strings that are zero edits away from word (i.e., just word itself).'\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    'Return all strings that are two edits away from this pinyin.'\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)} #要计算编辑距离为2的词 可以通过计算与原词编辑距离为1的词的编辑距离为1的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(word): #pinyin candidates piyin pingyin aogyin .... \n",
    "    'Find the most possible pinyin based on edit distance'\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwist default to word itself\n",
    "    candidates = (known(edits0(word)) or\n",
    "                  known(edits1(word)) or\n",
    "                  known(edits2(word)) or\n",
    "                  [word])  #计算所有与输入词编辑距离为0，1，2的词\n",
    "    return max(candidates,key=PINYIN_COUNT.get) #取出在所有candidate里在语料库里出现次数最多的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xin hua she b bo dao'"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_results(string):\n",
    "    pinyin = split_pinyin(string)[0]\n",
    "    pinyin = pinyin.split()\n",
    "    pinyin = map(correct, pinyin)\n",
    "    return (\" \").join(pinyin)\n",
    "parse_results(\"xinhuashebbodao\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
